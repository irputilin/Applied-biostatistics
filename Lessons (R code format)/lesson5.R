# Подгружайте свои данные и продолжаем работу

df <- read.csv('babies.csv')
df <- na.omit(df) # Если требуется

#### SUMMARY STATISTICS ####

# Термин "описательная статистика" подразумевает под собой расчёт некоторых
# статистических показателей, которые, так или иначе будут описывать распределение
# данных. В частности, вы можете помнить такие показатели, как среднее, медиана,
# квартили и т.д.
# Конечно, можно пользоваться только отдельными функциями для каждого отдельного
# вектора значений, но это будет занимать кучу времени. Для того и существует 
# R, чтобы облегчить нашу многотрудную жизнь. Прежде всего, опробуем функцию
# sapply(). sapply - одна из множества функций семейства "apply" - самая простая
# в использовании. Simplify and apply. Общий вид:
# 
#     sapply(набор_данных, применяемая функция)
#
# Для использования достаточно указать нужную таблицу и функцию, которую
# нужно применить к каждому столбцу

sapply(df, mean) 
sapply(df, median)

# Вы можете обратить внимание, что результат (конкретные числа) разных функций 
# округляются по-разному. Чтобы округлить число нужным вам образом воспользуйтесь
# функцией round(). Общий вид:
# 
#     round(вектор_данных, кол-во_знаков_после_запятой)

round(sapply(df, mean), 3)    # округляет до трёх знаков
round(sapply(df, mean), 1)    # округляет до одного знака


# Помимо функции sapply() для более подробного описания распределения данных
# в "один подход" можно воспользоваться функцией summary(), которая определит
# квартили и среднее значения распределения. Если на вход функции подать 
# таблицу, то она расчитает вышеуказанные показатели для каждого столбца

summary(df$weight)
summary(df)

# Может возникнуть ситуация, когда нужно наглядно понять, например, среднее значение
# какого-либо распределения и стандартное отклонение. Для этого воспользуемся функциями
# paste() и paste0()
# По сути, эти функции способны сделать буквенный элемент из всего, что ваша душа
# пожелает. Только функция paste сделает это с пробелами, а функция paste0 без 
# пробелов

paste(mean(df$weight), '±', sd(df$weight))
paste0(mean(df$weight), '±', sd(df$weight))

# Не забываем про функцию round

paste(round(mean(df$weight), 2), '±', round(sd(df$weight), 2))
paste0(round(mean(df$weight), 2), '±', round(sd(df$weight), 2))

# Периодически, может возникнуть надобность посчитать такой показатель, как
# коэффициент вариации (CV) - относительный показатель разброса данных, в 
# рамках которого стандартное отклонение выражается как процент от среднего
# Также, для удобства расчёта для всех столбцов таблицы воспольуемся функцией
# colMeans(), которая считает средние значения по каждому столбцу.

100*sapply(df, sd)/colMeans(df) 

# Помимо вывода полученных результатов в консоль, мы можем создавать для этих
# результатов отдельные таблицы. Так анализировать полученные результаты
# гораздо нагляднее. Воспользуемся функцией data.frame(). Общий вид:
#     
#     data.frame(название1 = данные1, название2 = данные2 ...)

desc_stat <- data.frame(Mean = colMeans(df),
                        CV = 100*sapply(df, sd)/colMeans(df) )

# У нас создалась таблица, в которой названия строк - названия столбцов в 
# анализируемой таблице. Второй столбец - Mean - средние значения по каждому
# столбцу исходной таблицы. Третий столбец - CV - коэффициент вариации 
# Но помните - R - язык программирования, созданные для задач статистической
# обработки данных и было бы странно, если бы для него не было написано
# конкретных решений для расчёта описательных статистик. 
# Чтобы облегчить себе жизнь вам необходимо установить библиотеку 
# psych (очень полезная библиотека, "джентельменский набор" каждого человека,
# который анализирует данные в R) и воспользоваться функцией describe()

install.packages('psych')   # Достаточно выполнить эту строку один раз.
                            # После закрытия RStudio библиотека останется
                            # установленной 

library(psych)

# Чтобы понять, какие параметры я использовал (и для чего) в данной функции
# воспользуйтесь справкой: ?describe

describe(df, skew = T, quant = c(0.025, 0.25, 0.5, 0.75, 0.972))

# Результат выполнения функции можно сохранить как таблицу с помощью функции
# as.data.frame (перезапишем уже имеющуся таблицу):
  
desc_stat <- describe(df, skew = T, quant = c(0.025, 0.25, 0.5, 0.75, 0.972))

# На примере использования функции describe есть смысл ознакомить вас с циклизацией.
# В частности, с использованием цикла for. 
# Представим, что в ваших данных имеются групповые переменные (а они должны быть).
# Чтобы не писать функцию describe() несколько раз для каждой исследемой группы
# как в примере ниже:

describe(df[df$smoke==1,], skew = T, quant = c(0.025, 0.25, 0.5, 0.75, 0.972))
describe(df[df$smoke==0,], skew = T, quant = c(0.025, 0.25, 0.5, 0.75, 0.972))

# (в моих данных группирующая переменная представляет себя и бинарную, поэтому
# пример получился не очень эффектным). Чтобы он стал эффектнее, создадим 
# факторную переменную на основе любой имеющейся:

df$groups <- cut(df$weight, breaks = 10, labels = c(1:10))

# Соответственно, попытка использования функции describe для всех 10 имеющихся
# групп будет выглядеть следующим образом:
  
describe(df[df$groups==1,], skew = T, quant = c(0.025, 0.25, 0.5, 0.75, 0.972))
describe(df[df$groups==2,], skew = T, quant = c(0.025, 0.25, 0.5, 0.75, 0.972))
describe(df[df$groups==3,], skew = T, quant = c(0.025, 0.25, 0.5, 0.75, 0.972))
describe(df[df$groups==4,], skew = T, quant = c(0.025, 0.25, 0.5, 0.75, 0.972))
...
describe(df[df$groups==10,], skew = T, quant = c(0.025, 0.25, 0.5, 0.75, 0.972))

# Общий вид цикла for выглядит следующим образом:

i = 1   # Ни к чему не обязывающая строка, просто создаём "итератор" для облегчения
        # написания цикла

for(указание_конкретных_данных){
  перечисление_действий
}

# Сделаем так, чтобы функция describe выбирала одну из имеющихся десяти групп
# в моих данных. Также, будем записывать полученные таблицы с описательными
# статистиками не просто в какую-то переменную, а непосредственно в свою рабочую
# директорую с помощью функции write.csv()

i = 1
for (i in unique(df$groups)){ # i будет принимать значения от 1 до 10
  temp <- subset(df, df$groups == i) # Функция subset создаёт "поднабор" данных
                                     # Будем записывать этот поднабор в отдельную
                                     # таблицу, которая будет меняться в зависимости
                                     # от того, чему равен i
  write.csv(describe(temp, skew = T), 
            paste0('descr_stat_', i, '_group'))
}

# Обратите внимание на функцию write.csv. Первым параметром мы указали результат
# выполнения функции describe, вторым параметром - название файла, которое генерируется
# с использованием конкретного значения i с помощью функции paste0
# После выполнения цикла обратите внимание на свою рабочую директорию. Там появились
# новые файлы!


#### NORMALITY TEST ####

# На прошлом занятии мы с вами разбирали, как визуально (самым простым способом)
# можно понять, насколько распределение данных соответствует нормальному.
# В этот раз дополним визуальные методы одним более точным (если можно так 
# выразиться) визуальным методом, а также разберём конкретные статистические
# тесты
# 
# Более точный визуальный метод проверки распределения на нормальность - 
# квантиль-квантильный график (QQ-plot). Как вы должны помнить (из лекции) 
# - QQplot позволяет визуально оценить отклонение данных от теоретического. 
# Чтобы построить такой график потребуется всего лишь две функции:
  
qqnorm(df$weight)
qqline(df$weight)

# По моим данным получилось, что точки не "ложатся" на прямую, соответственно,
# я делают вывод, что данные распределены, скорее всего, не нормально. Для того,
# чтобы показать вам, что имеется ввиду, посмотрите на следующий график, построенным
# по дата фрему mtcars

qqnorm(mtcars$disp)
qqline(mtcars$disp)

### TESTS ###

# Прежде чем перейдём к статистическим тестам, позволю себе небольшую ремарку.
# Далее, у вас может возникнуть путаница между словами "тест" и "критерий".
# Фактически, я не буду проводить между ними разницу, однако стоит помнить, что 
# статистический критерий - конкретное математическое правило, статистический
# тест - процедура, направленная на проверку гипотез с использованием конкретного
# критерия
# 
# Самый простой (математически и фактически) - t-критерий. Чаще всего используется
# t-критерий Стьюдента, однако, забегая немного вперёд, он используется для
# сравнения средних значений в двух выборках и при довольно специфичных условиях.
# В реальности же, данных критерий можно использовать как своего рода проверку
# на нормальность, используя для сравнения фактическое среднее значение выборки.
# К тому же, в 99,9999.... процентов случаев вы будете пользоваться не t-критерием
# Стьюдента, а t-критерием Уэлча, т.к. он не чуствителен к неравенству дисперсий, но
# об этом позже

# Из лекции вы должны помнить, что статистические тесты используются для того,
# чтобы понять принимать ли альтернативную гипотезу (Н1) или же оставаться при своём
# и быть уверенным в нулевой гипотезе (Н0). По простому: НО - данные не отличаются,
# Н1 - данные отличаются. Переводя данные гипотезы на язык проверки на нормальность:
# Н0 - данные распределены нормально, Н1 - данные распределены не нормально.
# Как понять, какой гипотезе "доверять"? Для этого существует p-value - probability
# value. В мировой науке (а также в аналитике данных и прочих областях жизни, где
# требуется статистический анализ) используется 95% интервал. Это значит, что
# вы закладываете в свои данные 5% на то, что они скорее всего какие-то неправильные.
# P-value существует для того, чтобы демонстрировать, является ли результат
# конкретного теста статистически значимым (p-value < 0.05 (или p-value < 5%))
# или нет (p-value >= 0.05)

# Чтобы использовать t-критерий для проверки выборки на нормальность воспользуемся
# функцией t.test()

t.test(df$weight, mu = median(df$weight))

# При проверке на нормальность с помощью t-теста необходимо указывать параметр
# mu - то среднее значение, которое мы считаем. Для верности, укажем медианное

# По моим данным получилось следующее:
  
#       One Sample t-test
# 
# data:  df$weight
# t = 5.7486, df = 1173, p-value = 1.146e-08
# alternative hypothesis: true mean is not equal to 125
# 95 percent confidence interval:
#   127.2914 129.6660
# sample estimates:
# mean of x 
# 128.4787 
  
# P-value получилось равным 1.146e-08. Это значит, что p-value = 1.146 в степени
# -8. Понятно, что данное значение намного меньше, чем 0.05. 
# Следовательно, я могу сделать вывод, что данные 
# распределены НЕ нормально (принимаем альтернативную гипотезу). Чтобы не 
# запутаться, зачастую при выполнении тестов альтернаятивная гипотеза 
# выводится в консоль

# Помимо специфического (для данной задачи) t-теста существуют и более классические,
# в частности, тест Шапиро-Уилка. Этот тест проверяет соответствие выборочных
# квантилей теоретическим квантилям
  
shapiro.test(df$weight)

# В моём случае, p-value опять получилось гораздо меньше, чем 0.05. В рамках данного
# теста H0 - данные распределены нормально, Н1 - данные распределены НЕ
# нормально. Делаем вывод, что второму из двух проведённх тестов распределение
# моих данных не соответствует нормальному. 

# Попробуем ещё один тест, который являлся одним из родоначальников всех критериев
# проверки распределения данных на нормальность - тест Харке-Бэра. Для этого вам
# необходимо установить библиотеку tseries. Эта библиотека будет также крайне 
# полезна, если мы с вами доберёмся до особого типа данных - временных серий.
# Этот критерий хорош тем, что для проверки на нормальность использует такие
# показатели распределения, как ассиметрия и эксцесс. Однако, в сравнении с многими
# другими критериями он обладает довольно "слабой" статистической силой.

library(tseries)

jarque.bera.test(df$weight)

# И вновь, по мои данные распределены НЕ нормально. P-value < 0.05.

# ... И опять я о своём ... Чтобы облегчить себе жизнь и не путаться со всеми
# этими числами и просто мгновенно понимать, как распределены данные мы можем
# самостоятельно написать функцию, которая исходя из результатов выполнения
# теста на нормальность будет говорить, как распределены данные

# Общий вид написаняия своей собственной функции:
  
название_функции <- function(переменные){
  тело_функции
}

# Напишем функцию, которая будет выполнять тест Шапиро-Уилка и говорить, как
# распределены данные

normality <- function(data){
  ifelse(shapiro.test(data)$p.value < 0.05, 'НЕ НОРМАЛЬНО', "НОРМАЛЬНО")
}

# Проверим её работоспособность:
  
normality(df$weight)

# В моём случае, результат получился "НЕ НОРМАЛЬНО".
# Функцию можно усложнить, например, указать, а какой статистический тест
# использовать для проверки на нормальность. Пусть это будет что-то одно на выбор:
# тест Шапиро-Уилка либо тест Харке-Бэра

normality2 <- function(data, test_type){
  if (test_type == 'shapiro'){
    ifelse(shapiro.test(data)$p.value < 0.05, 'НЕ НОРМАЛЬНО', "НОРМАЛЬНО")
  } else {
    if (test_type == 'jarque'){
      ifelse(jarque.bera.test(data)$p.value < 0.05, 'НЕ НОРМАЛЬНО', "НОРМАЛЬНО")
    } else {
      print('Use on of this: shapiro, jarque')
    }
  }
}

normality2(df$weight, 'shapiro')
normality2(df$weight, 'jarque')
